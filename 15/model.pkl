# Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import pickle

# Load the transformed and selected features from features.csv
features_df = pd.read_csv("features.csv")

# Split the data into training and testing sets
X = features_df.drop(["CPC", "CPA"], axis=1) # Independent variables (excluding CPC and CPA as they are not directly related to ad performance)
y = features_df[["CPC", "CPA"]] # Dependent variables (CPC and CPA as the target variables to optimize)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define a function to evaluate different models using various metrics
def evaluate_model(model, X_train, X_test, y_train, y_test):
    # Fit the model on the training data
    model.fit(X_train, y_train)
    
    # Predict on the testing data
    y_pred = model.predict(X_test)
    
    # Calculate the metrics for CPC and CPA separately
    for i, col in enumerate(["CPC", "CPA"]):
        mse = mean_squared_error(y_test[col], y_pred[:, i]) # Mean squared error
        mae = mean_absolute_error(y_test[col], y_pred[:, i]) # Mean absolute error
        rmse = np.sqrt(mse) # Root mean squared error
        r2 = r2_score(y_test[col], y_pred[:, i]) # R-squared score
        
        # Print the metrics for each column
        print(f"{col} Metrics:")
        print(f"MSE: {mse:.2f}")
        print(f"MAE: {mae:.2f}")
        print(f"RMSE: {rmse:.2f}")
        print(f"R2: {r2:.2f}")
        
        # Plot the actual vs predicted values for each column
        plt.scatter(y_test[col], y_pred[:, i])
        plt.plot([y_test[col].min(), y_test[col].max()], [y_test[col].min(), y_test[col].max()], color="red")
        plt.xlabel("Actual")
        plt.ylabel("Predicted")
        plt.title(f"Actual vs Predicted {col}")
        plt.show()
    
# Create different models to compare
lin_reg = LinearRegression() # Linear regression
dtree = DecisionTreeRegressor() # Decision tree
rf = RandomForestRegressor() # Random forest
xgb = XGBRegressor() # XGBoost

# Evaluate each model using the function defined above
print("Linear Regression:")
evaluate_model(lin_reg, X_train, X_test, y_train, y_test)

print("Decision Tree:")
evaluate_model(dtree, X_train, X_test, y_train, y_test)

print("Random Forest:")
evaluate_model(rf, X_train, X_test, y_train, y_test)

print("XGBoost:")
evaluate_model(xgb, X_train, X_test, y_train, y_test)

# Based on the evaluation results and business objectives, choose the best model and save it in a file named model.pkl
# For example, if we want to minimize the MSE and MAE for both CPC and CPA, we can choose the random forest model as it has the lowest values among the four models

best_model = rf # Random forest as the best model

# Save the best model in a file named model.pkl using pickle
with open("model.pkl", "wb") as f:
    pickle.dump(best_model, f)
